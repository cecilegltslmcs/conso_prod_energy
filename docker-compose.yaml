version: '3.9'

services:
  ingestion:
    build:
      context: /src/batch_layer/
      dockerfile: Dockerfile_ingestion
    volumes:
      - data:/data/raw/
    networks:
      - batch_layer

  postgres:
    image: postgres
    restart: always
    container_name: pg-database
    ports:
      - "5432:5432"
    environment: 
      - POSTGRES_DB = energy_consumption
    env_file:
      - .env_postgres
    volumes:
      - pgdata:/data/db
    networks:
      - batch_layer
    depends_on:
      - ingestion
 
  streamlit-batch:
    container_name: streamlit-batch
    build:
      context: /src/batch_layer/
      dockerfile: Dockerfile_dashboard-batch
    ports:
      - "8501:8501"
    networks:
      - batch_layer
    depends_on:
      - ingestion
      - postgres
 
  zookeeper:
    image: 'bitnami/zookeeper:latest'
    container_name: zookeeper
    networks:
      - streaming_layer
    ports:
      - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes

  kafka:
    image: 'bitnami/kafka:latest'
    container_name: kafka_server
    networks:
      - streaming_layer
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=CLIENT
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
    depends_on:
      - zookeeper

  kafka-producer-app:
    container_name: kafka_producer_app
    build:
      context: /src/streaming_layer/
      dockerfile: Dockerfile_kafka-component
    networks:
      - streaming_layer
    depends_on:
      - kafka

  spark:
    container_name: spark
    build:
      context: /src/streaming_layer/
      dockerfile: Dockerfile_spark-component
    hostname: spark_master
    user: root
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - streaming_layer
    depends_on:
      - kafka

  spark-worker:
    build:
      context: /src/streaming_layer/
      dockerfile: Dockerfile_spark-component
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no  
    networks:
      - streaming_layer
    depends_on:
      - spark

  spark-app:
    build:
      context: /src/streaming_layer/
      dockerfile: Dockerfile_spark-job
    networks:
      - streaming_layer
    depends_on:
      - kafka
      - spark

  mongo:
    container_name: mongo
    image: mongo:latest
    restart: always
    ports:
      - "27017:27017"
    networks:
      - streaming_layer
    environment:
      - MONGO_INITDB_DATABASE = energy_consumption
    env_file:
      - .env_mongo
    volumes:
      - mongodata:/data/db
    depends_on:
      - spark-app

  streamlit-stream:
    container_name: streamlit-stream
    build:
      context: /src/streaming_layer/
      dockerfile: Dockerfile_dashboard-stream
    ports:
      - "8502:8502"
    networks:
      - streaming_layer
    depends_on: 
      - mongo
    
volumes:
    data:
    pgdata:
    mongodata:
networks:
    batch_layer:
    streaming_layer:
